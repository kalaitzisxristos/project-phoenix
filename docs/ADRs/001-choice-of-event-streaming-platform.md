# ADR 001: Use Apache Kafka for the Core Transaction Stream

**Status**: Accepted

**Date**: 2025-08-04

## Context

The core business function of the Phoenix platform is to simulate and process a high-volume stream of financial transactions in real-time. The Transaction Simulator service needs to read from a dataset and publish each transaction to a message bus, simulating a live payment stream. The Fraud Detection service must then consume these messages for analysis. A definitive choice of technology is required to handle this critical data backbone.

## Decision

We will use **Apache Kafka** as the event streaming platform for the project. All financial transactions generated by the `simulator-transactions` service will be published to a dedicated `transactions` Kafka topic. The `fraud-detection-service` will be configured as a Kafka consumer subscribed to this topic.

## Consequences

* **Positive**:
    * **Alignment with Use Case**: Kafka is designed as a distributed, high-throughput, and replayable event log, which is the ideal paradigm for a continuous stream of financial transactions, as opposed to a traditional message broker.
    * **Enables Core Features**: This choice directly enables the implementation of the Fraud Detection service's real-time consumption logic and the Business Intelligence Dashboard, which will monitor the stream.
    * **Industry Standard**: Kafka is the de facto standard for event-driven architectures in modern fintech and high-growth tech companies, aligning the project with relevant industry practices.
    * **Scalability**: Kafka provides a clear path for horizontal scaling as the volume of transactions increases, which is a key architectural goal.

* **Negative**:
    * **Complexity**: Kafka has a higher operational complexity compared to simpler message brokers. This is mitigated by using a managed Helm chart for local deployment, which abstracts away much of the setup complexity.